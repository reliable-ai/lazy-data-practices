{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_settings = {\n",
    "    \"dataset\": [\n",
    "        \"ORG-017-DS-0\",\n",
    "        \"ORG-017-DS-2\",\n",
    "    ],\n",
    "    \"protected_attrs\": [\n",
    "        [\"age\"],\n",
    "        [\"job\"],\n",
    "        [\"job_and_age\"],\n",
    "        [\"marital_status\"],\n",
    "    ],\n",
    "    \"process_protected\": [\n",
    "        \"none\",\n",
    "        \"marital_status == married\",\n",
    "        \"age >= 35; job: privileged vs. unprivileged\",\n",
    "        \"age >= 35\",\n",
    "        \"age >= 25\",\n",
    "        \"age >= 25 and age < 60\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For trying out a particular setting\n",
    "chosen_settings = {\n",
    "    \"dataset\": \"ORG-017-DS-0\",\n",
    "    \"protected_attrs\": [\"age\"],\n",
    "    \"process_protected\": \"?\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write single function to run full analysis?\n",
    "- get propper settings\n",
    "- preprocess protected\n",
    "- select features\n",
    "- one-hot encode all categorical variables\n",
    "- fit different models\n",
    "- calculate fairness & performance metrics\n",
    "- determine best model (based on fairness & performance metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    ")\n",
    "from fairlearn.metrics import (\n",
    "    true_positive_rate,\n",
    "    true_negative_rate,\n",
    "    false_positive_rate,\n",
    "    false_negative_rate,\n",
    "    selection_rate,\n",
    "    count,\n",
    ")\n",
    "from fairlearn.metrics import (\n",
    "    equalized_odds_difference,\n",
    "    equalized_odds_ratio,\n",
    "    demographic_parity_difference,\n",
    "    demographic_parity_ratio,\n",
    ")\n",
    "\n",
    "metrics_to_calculate = {\n",
    "    \"accuracy\": accuracy_score,\n",
    "    \"balanced accuracy\": balanced_accuracy_score,\n",
    "    \"f1\": f1_score,\n",
    "    \"precision\": precision_score,\n",
    "    \"true positive rate\": true_positive_rate,\n",
    "    \"true negative rate\": true_negative_rate,\n",
    "    \"false positive rate\": false_positive_rate,\n",
    "    \"false negative rate\": false_negative_rate,\n",
    "    \"selection rate\": selection_rate,\n",
    "    \"count\": count,\n",
    "}\n",
    "fairness_metrics_to_calculate = {\n",
    "    \"equalized_odds_difference\": equalized_odds_difference,\n",
    "    \"equalized_odds_ratio\": equalized_odds_ratio,\n",
    "    \"demographic_parity_difference\": demographic_parity_difference,\n",
    "    \"demographic_parity_ratio\": demographic_parity_ratio,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "def hash_df(df):\n",
    "    return hashlib.md5(df.to_csv().encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autogluon.tabular.configs.hyperparameter_configs import get_hyperparameter_config\n",
    "\n",
    "GENERATED_DATA_DIR = Path(\"./generated_data\")\n",
    "GENERATED_DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def conduct_analysis(settings, seed = 80539):\n",
    "    print(f\"Running Analysis w/ Settings: {str(settings)}\")\n",
    "\n",
    "    # Set seed to zip code of our institute :)\n",
    "    np.random.seed(seed)\n",
    "    seed_2 = np.random.randint(0, 2**32 - 1)\n",
    "    random.seed(seed_2)\n",
    "    seed_3 = np.random.randint(0, 2**32 - 1)\n",
    "    torch.manual_seed(seed_3)\n",
    "    seed_4 = np.random.randint(0, 2**32 - 1)\n",
    "\n",
    "\n",
    "    # Load dataset\n",
    "    if settings[\"dataset\"] == \"ORG-017-DS-0\":\n",
    "        df = pd.read_csv(\"data/raw/bank-additional-full.csv\", sep=\";\")\n",
    "    elif settings[\"dataset\"] == \"ORG-017-DS-2\":\n",
    "        df = pd.read_csv(\"data/raw/bank-full.csv\", sep=\";\")\n",
    "    col_target = \"y\"\n",
    "\n",
    "    # Pre-process protected attribute(s)\n",
    "    if settings[\"process_protected\"] == \"age >= 35\":\n",
    "        df[\"age\"] = df[\"age\"] >= 35\n",
    "    elif settings[\"process_protected\"] == \"age >= 25\":\n",
    "        df[\"age\"] = df[\"age\"] >= 25\n",
    "    elif (settings[\"process_protected\"] == \"age >= 25 and age < 60\"):\n",
    "        df[\"age\"] = (df[\"age\"] >= 25) & (df[\"age\"] < 60)\n",
    "    elif (settings[\"process_protected\"] == \"?\"):\n",
    "        # Mirror most common decision on unknown processing\n",
    "        df[\"age\"] = (df[\"age\"] >= 25) & (df[\"age\"] < 60)\n",
    "    elif settings[\"process_protected\"] == \"age >= 35; job: privileged vs. unprivileged\":\n",
    "        df[\"age\"] = df[\"age\"] >= 35\n",
    "\n",
    "        df[\"job\"] = df[\"job\"].replace({\n",
    "            'management': \"privileged\",\n",
    "            'technician': \"privileged\",\n",
    "            'admin.': \"privileged\",\n",
    "            'self-employed': \"privileged\",\n",
    "            'entrepreneur': \"privileged\",\n",
    "            'blue-collar': \"unprivileged\",\n",
    "            'services': \"unprivileged\",\n",
    "            'retired': \"unprivileged\",\n",
    "            'unemployed': \"unprivileged\",\n",
    "            'housemaid': \"unprivileged\",\n",
    "            'student': \"unprivileged\",\n",
    "            'unknown': \"unprivileged\",\n",
    "        })\n",
    "\n",
    "        # Create a new attribute that identifies the most unprivleged group\n",
    "        # as a combination of being in the vulnerable group for both age and job\n",
    "        df[\"job_and_age\"] = (df[\"job\"] == \"privileged\") | df[\"age\"]\n",
    "\n",
    "        # Drop the original job and age attributes since we cannot exclude them\n",
    "        # by default as it would mess with how the fairness metric is computed.\n",
    "        df.drop(columns=[\"job\", \"age\"], inplace=True)\n",
    "    elif settings[\"process_protected\"] == \"marital == married\":\n",
    "        df[\"marital\"] = df[\"marital\"] == \"married\"\n",
    "    elif settings[\"process_protected\"] == \"none\":\n",
    "        pass\n",
    "    else:\n",
    "        raise \"Unsupported value for setting process_protected\"\n",
    "\n",
    "    # Use all features\n",
    "    col_features = df.columns\n",
    "    print(f\"All columns: {col_features}\")\n",
    "    # Remove protected attributes as possible features\n",
    "    col_features = set(col_features) - set(settings[\"protected_attrs\"])\n",
    "    col_features.remove(col_target)\n",
    "\n",
    "    # ==== One-hot Encoding ====\n",
    "    # Identify all non-numeric columns in df\n",
    "    categorical_features = df[list(col_features)].select_dtypes(include=[\"object\"]).columns\n",
    "    # One-hot encode all categorical features\n",
    "    df_onehot = pd.get_dummies(df[categorical_features])\n",
    "    print(df_onehot)\n",
    "    new_features_onehot = df_onehot.columns\n",
    "    # Add one-hot encoded features\n",
    "    df = df.join(df_onehot)\n",
    "\n",
    "    # Use the one-hot encoded features instead of the original ones\n",
    "    col_features = set(col_features) | set(new_features_onehot)\n",
    "    col_features = set(col_features) - set(categorical_features)\n",
    "\n",
    "    print(f\"Features: {col_features}\")\n",
    "\n",
    "    # Prepare final data\n",
    "    final_data = df[sorted(col_features)]\n",
    "    # Turn target into a binary (i.e. not string)\n",
    "    final_target = df[col_target] == \"yes\"\n",
    "    final_protected  = df[settings[\"protected_attrs\"]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test, prot_train, prot_test = train_test_split(\n",
    "        final_data,\n",
    "        final_target,\n",
    "        final_protected,\n",
    "        random_state=seed_4\n",
    "    )\n",
    "\n",
    "    label = \"target\"\n",
    "    full_train = X_train.copy()\n",
    "    full_train[label] = y_train\n",
    "    full_test = X_test.copy()\n",
    "    full_test[label] = y_test\n",
    "\n",
    "    ds_train = TabularDataset(full_train)\n",
    "\n",
    "    hyperparameters = get_hyperparameter_config(\"default\")\n",
    "    hyperparameters[\"LR\"] = {}\n",
    "\n",
    "    predictor = TabularPredictor(\n",
    "        label=label\n",
    "    ).fit(\n",
    "        ds_train,\n",
    "        hyperparameters=hyperparameters\n",
    "    )\n",
    "    y_pred = predictor.predict(TabularDataset(X_test))\n",
    "    print(predictor.leaderboard(full_test))\n",
    "\n",
    "    output = {}\n",
    "\n",
    "    # Combine all settings-values into one string\n",
    "    settings_string = '-'.join(str(value) for value in settings.values())\n",
    "\n",
    "    X_train.to_csv(GENERATED_DATA_DIR / f\"{settings_string}-{seed}-X_train.csv\")\n",
    "    X_test.to_csv(GENERATED_DATA_DIR / f\"{settings_string}-{seed}-X_test.csv\")\n",
    "    final_data.to_csv(GENERATED_DATA_DIR / f\"{settings_string}-{seed}-X_all.csv\")\n",
    "\n",
    "    final_all_combined_again = pd.concat([\n",
    "        final_data,\n",
    "        final_target.to_frame(\"target\"),\n",
    "        final_protected,\n",
    "    ], axis=1, ignore_index=False)\n",
    "    final_all_combined_again.to_csv(GENERATED_DATA_DIR / f\"{settings_string}-full_data.csv\")\n",
    "\n",
    "    base_rates = final_protected.value_counts().to_dict()\n",
    "    hash_train = hash_df(X_train)\n",
    "    hash_test = hash_df(X_test)\n",
    "    for name in predictor.model_names():\n",
    "\n",
    "        y_pred = predictor.predict(TabularDataset(X_test), model = name)\n",
    "\n",
    "        # Prepare output dict with metrics etc.\n",
    "        output[name] = {\n",
    "            'model': name,\n",
    "            'seed': seed,\n",
    "            'settings': settings,\n",
    "            'base_rates': base_rates,\n",
    "            'hash_train': hash_train,\n",
    "            'hash_test': hash_test,\n",
    "        }\n",
    "        # Compute metrics\n",
    "        standard_metrics = {\n",
    "            name: metric(\n",
    "                y_true=y_test,\n",
    "                y_pred=y_pred,\n",
    "            )\n",
    "            for name, metric in metrics_to_calculate.items()\n",
    "        }\n",
    "        output[name].update(standard_metrics)\n",
    "        fairness_metrics = {\n",
    "            name: metric(\n",
    "                y_true=y_test,\n",
    "                y_pred=y_pred,\n",
    "                sensitive_features=prot_test,\n",
    "            )\n",
    "            for name, metric in fairness_metrics_to_calculate.items()\n",
    "        }\n",
    "        output[name].update(fairness_metrics)\n",
    "\n",
    "    # Generate dataframe from output dict\n",
    "    df_output = pd.DataFrame.from_dict(output).transpose()\n",
    "\n",
    "    # Clean up model files\n",
    "    # Let's prefix the predictor path with \"local directory\",\n",
    "    # just to be extra safe with the recursive delete :)\n",
    "    shutil.rmtree(Path(f\"./{predictor.path}\"))\n",
    "\n",
    "    return df_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a single analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = conduct_analysis(settings = chosen_settings)\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run The Big Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "paper_cols = [\"new_dataset_id\", \"col_protected\", \"processing_protected\"]\n",
    "bank_papers = pd.read_csv(\"data/raw/bank-papers.csv\")\n",
    "\n",
    "# Process\n",
    "bank_papers[\"processing_protected\"].replace(\"age < 25 or age > 60\", \"age >= 25 and age < 60\", inplace=True) # Are we ok with this?\n",
    "bank_papers[\"processing_protected\"].fillna(\"none\", inplace=True)\n",
    "\n",
    "# Extract protected colnames\n",
    "bank_papers['col_protected'] = bank_papers['col_protected'].apply(json.loads).apply(lambda x: frozenset(x.keys()))\n",
    "bank_papers['col_protected'].replace({frozenset({\"age\", \"job\"}): frozenset({\"job_and_age\"})}, inplace=True)\n",
    "\n",
    "# Rename columns to match settings\n",
    "bank_papers.rename(columns={\n",
    "    \"new_dataset_id\": \"dataset\",\n",
    "    \"col_protected\": \"protected_attrs\",\n",
    "    \"processing_protected\":\n",
    "    \"process_protected\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Remove duplicate configurations (this needs frozenset or tuple to be hashable)\n",
    "bank_papers.drop_duplicates(inplace=True)\n",
    "\n",
    "bank_papers['protected_attrs'] = bank_papers['protected_attrs'].apply(list)\n",
    "\n",
    "bank_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_settings = bank_papers.to_dict(orient='records')\n",
    "observed_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "seeds = [\n",
    "    # Original Seed\n",
    "    80539,\n",
    "    # random.org 1 - 10000\n",
    "    8811,\n",
    "    1563,\n",
    "    9401,\n",
    "    3032,\n",
    "    8060,\n",
    "    2412,\n",
    "    6371,\n",
    "    9192,\n",
    "    115,\n",
    "]\n",
    "\n",
    "for seed in seeds:\n",
    "    for sett in observed_settings:\n",
    "        outputs.append(conduct_analysis(settings = sett, seed = seed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.concat(outputs)\n",
    "final_data.to_csv(\"bank-analysis-results.csv\")\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[\"accuracy\"].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness-datasets-xpEoPfN6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
